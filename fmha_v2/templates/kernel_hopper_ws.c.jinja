{{copyright}}

#include <fused_multihead_attention_utils.h>
#include <fmha/hopper/gmma_descriptor.h>
#include <fmha/hopper/smem_tile.h>
#include <fmha/utils.h>
#include <fmha/hopper/compute_tile.h>

#include <fmha/warpspec/kernel_traits.h>
#include <fmha/warpspec/dma.h>
#include <fmha/warpspec/compute.h>

{{include_str}}
////////////////////////////////////////////////////////////////////////////////////////////////////
{{local_ns_open}}
#if CUDA_VERSION >= {{min_cuda_version}}

static constexpr int DMA2COMPUTE_DEPTH = 1;
{{num_compute_groups_str}}
static constexpr bool USE_TMA_STORE = {{use_tma_store_flag}};

{{bert_launch_params}}
{{attn_mask_type_str}}

using Ktraits = {{kernel_traits_header}}
                {{loop_step}},
                {{kv_loop_step}},
                {{head_size}},
                {{head_size_v}},
                {{q_tile_buffers}},
                {{kv_tile_buffers}},
                NUM_COMPUTE_GROUPS,
                DMA2COMPUTE_DEPTH,
                0,
                {{heads_interleaved_flag}},
                false,
                {{enable_mutex_flag}},
                {{scheduling_mode}},
                {{input_layout_flag}},
                USE_TMA_STORE,
                {{enable_attn_logit_softcapping_flag}},
                {{return_softmax_stats_flag}},
                {{output_dtype_}},
                {{sage_block_size_q}},
                {{sage_block_size_k}},
                {{sage_block_size_v}}>;

using Ktraits_causal = {{kernel_traits_header}}
                       {{loop_step}},
                       {{kv_loop_step}},
                       {{head_size}},
                       {{head_size_v}},
                       {{q_tile_buffers}},
                       {{kv_tile_buffers}},
                       NUM_COMPUTE_GROUPS,
                       DMA2COMPUTE_DEPTH,
                       1,
                       {{heads_interleaved_flag}},
                       {{has_alibi}},
                       {{enable_mutex_flag}},
                       {{scheduling_mode}},
                       {{input_layout_flag}},
                       USE_TMA_STORE,
                       {{enable_attn_logit_softcapping_flag}},
                       {{return_softmax_stats_flag}},
                       {{output_dtype_}}>;

using Ktraits_sliding_or_chunked_causal = {{kernel_traits_header}}
                                      {{loop_step}},
                                      {{kv_loop_step}},
                                      {{head_size}},
                                      {{head_size_v}},
                                      {{q_tile_buffers}},
                                      {{kv_tile_buffers}},
                                      NUM_COMPUTE_GROUPS,
                                      DMA2COMPUTE_DEPTH,
                                      2,
                                      {{heads_interleaved_flag}},
                                      {{has_alibi}},
                                      {{enable_mutex_flag}},
                                      {{scheduling_mode}},
                                      {{input_layout_flag}},
                                      USE_TMA_STORE && false,
                                      {{enable_attn_logit_softcapping_flag}},
                                      {{return_softmax_stats_flag}},
                                      {{output_dtype_}}>;

using Ktraits_custom_mask = {{kernel_traits_header}}
                            {{loop_step}},
                            {{kv_loop_step}},
                            {{head_size}},
                            {{head_size_v}},
                            {{q_tile_buffers}},
                            {{kv_tile_buffers}},
                            NUM_COMPUTE_GROUPS,
                            DMA2COMPUTE_DEPTH,
                            3,
                            {{heads_interleaved_flag}},
                            {{has_alibi}},
                            {{enable_mutex_flag}},
                            {{scheduling_mode}},
                            {{input_layout_flag}},
                            USE_TMA_STORE && false,
                            {{enable_attn_logit_softcapping_flag}},
                            {{return_softmax_stats_flag}},
                            {{output_dtype_}}>;

////////////////////////////////////////////////////////////////////////////////////////////////////

#if {{padding_mask}} // padding_mask

using Shared = typename Ktraits::Shared;

extern "C"
__global__ __launch_bounds__(Ktraits::THREADS, 1)
void {{kernel_name}}(
    const __grid_constant__ {{params_type}} params){

    extern __shared__ char smem_[];
    char *smem_aligned = fmha::align_1024(smem_);

    Shared *shared = reinterpret_cast<Shared *>(&smem_aligned[0]);
    shared->init(threadIdx.x == 0);
    __syncthreads();

    // special trick to avoid wrap_sync (leads to illegal instruction)
    int warp_group = __shfl_sync(0xffffffff, threadIdx.x / 128, 0);
    int tidx = threadIdx.x % 128;

    if( warp_group == NUM_COMPUTE_GROUPS ) {  // dma + sched

        {{setmaxnreg_dma_str}}
        uint32_t elect_one = tidx == 0;

        // Need all threads involved when the dam group needs to transpose the v tile explicltly.
        if constexpr ( Ktraits::DMA_GROUP_TRANSPOSE_V ) {
            fmha::ws::DMA<Ktraits>::Device dma_device(elect_one);
            dma_device.{{run_fct_name}}(params, shared);
        } else {
            fmha::ws::DMA<Ktraits>::Device dma_device(elect_one);
            if( tidx < 32 ) {
                dma_device.{{run_fct_name}}(params, shared);
            }
        }

    } else {  // math

        {{setmaxnreg_compute_str}}

        fmha::ws::Compute<fmha::{{instruction_traits}}, Ktraits> compute;
        compute.run(warp_group, tidx, shared, params);
    }
}

#endif // padding mask

////////////////////////////////////////////////////////////////////////////////////////////////////

#if {{causal_mask}} // causal_mask

using Shared_causal = typename Ktraits_causal::Shared;

extern "C"
__global__ __launch_bounds__(Ktraits_causal::THREADS, 1)
void {{causal_kernel_name}}(
    const __grid_constant__ {{params_type}} params){

    extern __shared__ char smem_[];
    char *smem_aligned = fmha::align_1024(smem_);

    Shared_causal *shared = reinterpret_cast<Shared_causal *>(&smem_aligned[0]);
    shared->init(threadIdx.x == 0);
    __syncthreads();

    // special trick to avoid wrap_sync (leads to illegal instruction)
    int warp_group = __shfl_sync(0xffffffff, threadIdx.x / 128, 0);
    int tidx = threadIdx.x % 128;

    if( warp_group == NUM_COMPUTE_GROUPS ) {  // dma + sched

        {{setmaxnreg_dma_str}}
        uint32_t elect_one = tidx == 0;

        // Need all threads involved when the dam group needs to transpose the v tile explicltly.
        if constexpr ( Ktraits_causal::DMA_GROUP_TRANSPOSE_V ) {
            fmha::ws::DMA<Ktraits_causal>::Device dma_device(elect_one);
            dma_device.{{run_fct_name}}(params, shared);
        } else {
            fmha::ws::DMA<Ktraits_causal>::Device dma_device(elect_one);
            if( tidx < 32 ) {
                dma_device.{{run_fct_name}}(params, shared);
            }
        }

    } else {  // math

        {{setmaxnreg_compute_str}}

        fmha::ws::Compute<fmha::{{instruction_traits}}, Ktraits_causal> compute;
        compute.run(warp_group, tidx, shared, params);
    }
}

#endif // causal mask

////////////////////////////////////////////////////////////////////////////////////////////////////

#if {{sliding_or_chunked_causal_mask}} // sliding_or_chunked_causal_mask

using Shared_sliding_or_chunked_causal = typename Ktraits_sliding_or_chunked_causal::Shared;

extern "C"
__global__ __launch_bounds__(Ktraits_sliding_or_chunked_causal::THREADS, 1)
void {{sliding_or_chunked_causal_kernel_name}}(
    const __grid_constant__ {{params_type}} params){

    extern __shared__ char smem_[];
    char *smem_aligned = fmha::align_1024(smem_);

    Shared_sliding_or_chunked_causal *shared =
        reinterpret_cast<Shared_sliding_or_chunked_causal *>(&smem_aligned[0]);
    shared->init(threadIdx.x == 0);
    __syncthreads();

    // special trick to avoid wrap_sync (leads to illegal instruction)
    int warp_group = __shfl_sync(0xffffffff, threadIdx.x / 128, 0);
    int tidx = threadIdx.x % 128;

    if( warp_group == NUM_COMPUTE_GROUPS ) {  // dma + sched

        {{setmaxnreg_dma_str}}
        uint32_t elect_one = tidx == 0;

        // Need all threads involved when the dam group needs to transpose the v tile explicltly.
        if constexpr ( Ktraits_sliding_or_chunked_causal::DMA_GROUP_TRANSPOSE_V ) {
            fmha::ws::DMA<Ktraits_sliding_or_chunked_causal>::Device dma_device(elect_one);
            dma_device.{{run_fct_name}}(params, shared);
        } else {
            fmha::ws::DMA<Ktraits_sliding_or_chunked_causal>::Device dma_device(elect_one);
            if( tidx < 32 ) {
                dma_device.{{run_fct_name}}(params, shared);
            }
        }

    } else {  // math

        {{setmaxnreg_compute_str}}

        fmha::ws::Compute<fmha::{{instruction_traits}}, Ktraits_sliding_or_chunked_causal> compute;
        compute.run(warp_group, tidx, shared, params);
    }
}

#endif // sliding_or_chunked_causal_mask

////////////////////////////////////////////////////////////////////////////////////////////////////

#if {{custom_mask}} // custom_mask

using Shared_custom_mask = typename Ktraits_custom_mask::Shared;

extern "C"
__global__ __launch_bounds__(Ktraits_custom_mask::THREADS, 1)
void {{custom_mask_kernel_name}}(
    const __grid_constant__ {{params_type}} params){

    extern __shared__ char smem_[];
    char *smem_aligned = fmha::align_1024(smem_);

    Shared_custom_mask *shared =
        reinterpret_cast<Shared_custom_mask *>(&smem_aligned[0]);
    shared->init(threadIdx.x == 0);
    __syncthreads();

    // special trick to avoid wrap_sync (leads to illegal instruction)
    int warp_group = __shfl_sync(0xffffffff, threadIdx.x / 128, 0);
    int tidx = threadIdx.x % 128;

    if( warp_group == NUM_COMPUTE_GROUPS ) {  // dma + sched

        {{setmaxnreg_dma_str}}
        uint32_t elect_one = tidx == 0;

        // Need all threads involved when the dam group needs to transpose the v tile explicltly.
        if constexpr ( Ktraits_custom_mask::DMA_GROUP_TRANSPOSE_V ) {
            fmha::ws::DMA<Ktraits_custom_mask>::Device dma_device(elect_one);
            dma_device.{{run_fct_name}}(params, shared);
        } else {
            fmha::ws::DMA<Ktraits_custom_mask>::Device dma_device(elect_one);
            if( tidx < 32 ) {
                dma_device.{{run_fct_name}}(params, shared);
            }
        }

    } else {  // math

        {{setmaxnreg_compute_str}}

        fmha::ws::Compute<fmha::{{instruction_traits}}, Ktraits_custom_mask> compute;
        compute.run(warp_group, tidx, shared, params);
    }
}

#endif // custom_mask

////////////////////////////////////////////////////////////////////////////////////////////////////

void {{launcher_name}}(
    {{fused_multihead_attention_params_v2_str}} &params,
    const Launch_params &launch_params, cudaStream_t stream){

    {{TMA_config}}
    if( Ktraits::SCHEDULING_MODE > 0 ) {
        FMHA_CHECK_CUDA(cudaMemsetAsync(params.tile_id_counter_ptr, 0, sizeof(uint32_t), stream));
    }

    dim3 block_size;

    if( Ktraits::SCHEDULING_MODE == 0 ) {
        block_size.y = std::min(params.b * params.h, launch_params.multi_processor_count);
        // distribute m steps to multiple blocks (fully utilize SMs)
        // block.x = blocks that handle single head, block.y = blocks that handle different heads
        size_t sms_per_head = (launch_params.multi_processor_count) / block_size.y;
        // Take multiple compute groups into consideration.
        size_t m_steps = size_t((params.s + {{loop_step}} * NUM_COMPUTE_GROUPS - 1) / ({{loop_step}} * NUM_COMPUTE_GROUPS));

        // 2 * {{bytes_per_elt}} stands for kv cache and {{bytes_per_elt}} bytes per element.
        size_t size_in_bytes = block_size.y * params.s * params.d * 2 * {{bytes_per_elt}};
        if( size_in_bytes <= launch_params.device_l2_cache_size ) {
            // strategy 1: limit to only 1 wave
            block_size.x = std::min(m_steps, sms_per_head);
        } else {
            // strategy 2: fully unroll the q loops (contiguous blocks handle all q loops)
            block_size.x = m_steps;
        }
        params.num_tiles = params.b * params.h;
    } else if( Ktraits::SCHEDULING_MODE == 1 ) {
        // Get the max total M steps
        // Take multiple compute groups into consideration.
        size_t m_steps = size_t((params.s + {{loop_step}} * NUM_COMPUTE_GROUPS - 1) / ({{loop_step}} * NUM_COMPUTE_GROUPS));
        params.num_tiles_per_head = static_cast<uint32_t>(m_steps);
        params.num_tiles = static_cast<uint32_t>(m_steps * params.b * params.h);
        if (launch_params.attention_mask_type == Attention_mask_type::CAUSAL) {
            // 2 * {{bytes_per_elt}} stands for kv cache and {{bytes_per_elt}} bytes per element.
            size_t size_in_bytes = params.b * params.h * params.s * params.d * 2 * {{bytes_per_elt}};
            params.use_balanced_scheduling = (size_in_bytes <= launch_params.device_l2_cache_size);
        }

        block_size.x = 1;
        block_size.y = std::min(static_cast<int>(params.num_tiles), launch_params.multi_processor_count);
    } else {
        assert(false && "Invalid SCHEDULING_MODE");
    }

    // Reuse the same bytes_per_smem for launching kernels.
    constexpr int SMEM_BYTES = Ktraits::BYTES_PER_SMEM;
    if( launch_params.attention_mask_type == Attention_mask_type::PADDING ) {
#if {{padding_mask}} // padding_mask
        FMHA_CHECK_CUDA(cudaFuncSetAttribute({{kernel_name}},
                                         cudaFuncAttributeMaxDynamicSharedMemorySize,
                                         SMEM_BYTES));

        {{kernel_name}}
            <<<block_size, Ktraits::THREADS, SMEM_BYTES, stream>>>({{params_str}});
#endif // padding_mask
    } else if( launch_params.attention_mask_type == Attention_mask_type::CAUSAL ) {
#if {{causal_mask}} // causal_mask
        FMHA_CHECK_CUDA(cudaFuncSetAttribute({{causal_kernel_name}},
                                         cudaFuncAttributeMaxDynamicSharedMemorySize,
                                         SMEM_BYTES));

        {{causal_kernel_name}}
            <<<block_size, Ktraits::THREADS, SMEM_BYTES, stream>>>({{params_str}});
#endif // causal mask
    } else if( launch_params.attention_mask_type == Attention_mask_type::SLIDING_OR_CHUNKED_CAUSAL ) {
#if {{sliding_or_chunked_causal_mask}} // sliding_or_chunked_causal_mask
        FMHA_CHECK_CUDA(cudaFuncSetAttribute({{sliding_or_chunked_causal_kernel_name}},
                                         cudaFuncAttributeMaxDynamicSharedMemorySize,
                                         SMEM_BYTES));

        {{sliding_or_chunked_causal_kernel_name}}
            <<<block_size, Ktraits::THREADS, SMEM_BYTES, stream>>>({{params_str}});
#endif // sliding_or_chunked_causal_mask
    } else if( launch_params.attention_mask_type == Attention_mask_type::CUSTOM_MASK ) {
#if {{custom_mask}} // custom_mask
        FMHA_CHECK_CUDA(cudaFuncSetAttribute({{custom_mask_kernel_name}},
                                         cudaFuncAttributeMaxDynamicSharedMemorySize,
                                         SMEM_BYTES));

        {{custom_mask_kernel_name}}
            <<<block_size, Ktraits::THREADS, SMEM_BYTES, stream>>>({{params_str}});
#endif // custom mask
    }

}

#endif
{{local_ns_close}}